The Principal of Bospar and one of Business Insider’s "50 Best Public Relations People In The Tech Industry."   ChatGPT has been an absolute sensation since its launch in November 2022. It brings to bear an impressive array of capabilities, from writing software code to creating hipster craft cocktail recipes. Its generative artificial intelligence (AI) cousins, including Google’s Bard and Microsoft’s Bing AI, deliver similar features and functionality. At the core of that functionality—no matter what AI you choose—is the ability to take instructions (called prompts) and turn those into readable content. It can be used for a wide variety of purposes, but we all know that its most common application is that of writing. Reams of column inches have already been written about how it can save time on creating everything from memos to video scripts, press releases, poetry and more. It is also being used to write for media outlets and the broader internet, at times leading to pretty disastrous results and no small amount of controversy. Tech news website CNET took a swing at writing some of their content with ChatGPT—and didn’t bother to tell anyone about it. This resulted in significant backlash and negative media coverage, and it was revealed that 41 of the 77 articles that were written with AI—published under a rather opaque “staff” byline—contained factual errors, forcing the publication to issue multiple corrections. More recently, the German publication Die Aktuelle—which looks not too dissimilar to celeb-stalking rags like the National Enquirer—ran a story on seven-time Formula One champion Michael Schumacher, which was fabricated using AI. The phony “interview” (paywall) with the incapacitated former driver was called out by his family as being false. The parent company issued an apology, and the editor was rightfully removed from her job. Not inconsequentially, the publisher is now exposed to some meaningful legal liability, with a lawsuit potentially in the works. All of this evidence points to the fact that generative AI is already a boon for the bottom-feeders. This applies to the news industry, as well as the internet at large. Those who operate bots and content farms or write product descriptions and landing pages can now use ChatGPT and its cohort products to promulgate whatever reckless, inaccurate content that they wish, with little currently in the way of consequences. Surely AI tools are being used by bad actors to advance conspiracy theories and hate speech online, and many of these shady characters will seek to hide their use of AI and present it as genuine, original content. After all, a major publisher or media outlet might act responsibly when caught using AI—or admit it upfront—but millions, even billions, of other pages on the internet will likely get away with publishing dubious or outright inaccurate information generated by AI. Against this backdrop of huge turmoil and limited accountability, the journalism industry is trying to muster a response. Content publishers in the form of major media outlets are hoping that chatbot-assisted search will send additional users to their content, and journalism organizations (paywall) are now grappling with ethical considerations around the use of AI in the profession. In an era when print journalism is struggling financially, the ability of AI-powered search to direct traffic (and thus revenue) to the “big guys” could be the final nail in the coffin for many a local newspaper or fringe media outlet. To further explore this issue, my agency hosted a roundtable discussion at the San Francisco Press Club (of which I am president) with top journalists, as well as tech companies. Panelists included representatives from AFP, CNET/ZDNet, MarketWatch, Reuters, the San Francisco Examiner, VentureBeat, WIRED and Microsoft. Moderated by Rachel Metz of Bloomberg, the session looked at the most current topics at the intersection of journalism and AI. Key takeaways from the discussion, which had an audience of over 100 communications pros and media influencers, were that large news operations are already well into experimenting with the use of AI in gathering more information quickly. Use cases like developing “net new” products, as well as researching financial news and company data, are already being experimented with. At the same time, reputable news organizations are doing their best to tread carefully around issues like disinformation and accuracy, as well as disclosing to the audience that AI is being used for public-facing news content. While my own experience with using ChatGPT for specific elements of public relations (PR) and comms work has shown promise, I think it will be a number of years until AI does the heavy lifting in the area of writing and reporting. This also goes for both sides of the PR equation—which means that we’re far from using generative AI to write pitches and press releases and to solicit media coverage—which is, of course, PR’s bread and butter. Similarly, journalists shouldn’t be ready to surrender their jobs to news-reporting bots, because the accuracy and the trust simply are not there—not to mention the lack of human touch and the ability to have a “BS detector” when appropriate. Even as we lean into AI as an industry, numerous concerns remain. This means that we can enjoy some degree of near-term job security as we seek to figure out how to live with this highly disruptive technology, from both practical and ethical perspectives. In a rapidly changing world, that’s not necessarily a bad thing. Forbes Communications Council is an invitation-only community for executives in successful public relations, media strategy, creative and advertising agencies. Do I qualify? 