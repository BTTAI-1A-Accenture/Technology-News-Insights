Google CEO Sundar Pichai admitted Sunday that society will need to prepare for rapid advancements in artificial intelligence that are poised to unsettle the job market and exacerbate safety concerns. In an interview with CBS’s “60 Minutes” on Sunday night, Pichai called for new regulations to govern AI – even as Elon Musk and others call for an outright pause on the development of advanced AI systems due to safety concerns. “I think we have to be very thoughtful,” Pichai said. “And I think these are all things society needs to figure out as we move along. It’s not for a company to decide.” Pichai shocked “60 Minutes” anchor Scott Pelley with a number of bold proclamations about AI’s potential applications – declaring that it would affect “every product across every company” in the near future and even admitting at one point that Google did not fully understand all aspects of its own systems. Pichai said advanced artificial intelligence was mostly to cause job losses among so-called “knowledge workers,” such as writers, accountants, architects and software engineers. At one point during the wide-ranging conversation, Pichai addressed bizarre scenarios in which Google’s AI programs have developed “emergent properties” – or learned unanticipated skills in which they were not trained. In one case, a Google program developed the ability to translate the language Bengali – despite never being “taught” the dialect. Pichai admitted that his company’s engineers could not fully explain the phenomenon. “There is an aspect of this which we call, all of us in the field call it as a ‘black box,’” Pichai said. “You know, you don’t fully understand. And you can’t quite tell why it said this, or why it got [it] wrong.” The response prompted Pelley to question how Google could “turn it loose on society” without a total understanding of the technology. “Let me put it this way. I don’t think we fully understand how a human mind works either,” Pichai added. “AI will impact everything,” Pichai said. “For example, you could be a radiologist – if you think about five to ten years from now – you’re going to have an AI collaborator with you. You come in the morning, let’s say you have a hundred things to go through, it may say, ‘these are the most serious cases you need to look at first.’” Meanwhile, Google is scrambling to develop its own fledgling chatbot, “Bard,” to keep pace in the emerging market. Pichai admitted to having doubts about whether society was ready for the looming impact of AI. “There are two ways I think about it,” Pichai said. “On one hand I feel, no, because you know, the pace at which we can think and adapt as societal institutions, compared to the pace at which the technology’s evolving — there seems to be a mismatch.” “On the other hand, compared to any other technology, I’ve seen more people worried about it earlier in its life cycle – so I feel optimistic,” Pichai added. “The number of people, you know, who have started worrying about the implications, and hence the conversations are starting in a serious way as well.” The debate over AI safety has intensified in recent days due to the runaway success of Microsoft-backed OpenAI’s ChatGPT.  The chatbot has gained a massive following due to its humanlike responses to various prompts, even as it exacerbates concerns about potential job losses and the spread of misinformation. The contrast is on full display through the recent actions of billionaire Elon Musk – who recently warned that AI has the potential for “civilization destruction” even as he launches his own AI startup that will directly compete with ChatGPT. Last month, Musk joined with more than 1,000 experts in advocating for a six-month pause in AI development until proper guardrails are in place. Advertisement