
                Agree & Join LinkedIn
               
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
                Create your free account or sign in to continue your search
                 
              or
             
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
                New to LinkedIn? Join now
 
                  or
                 
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
              New to LinkedIn? Join now
 The earliest mention of a software engineering system that I found in research literature was a conference paper from 1979 which described a system that was used for developing mainframe operating systems by International Computers Limited [13]. The system, CADES, included a formal design methodology, a design definition language, a product database, formal data capture and controls, and a product database application. According to the publication, “The goal of this system was to save the company as much product development money and lead times as possible.” This system placed constraints on the software development process through tools, policies, or processes. In the process of working on this system they also discovered what sounds like dependency hell: “...we completely underestimated the magnitude of the multiple version problem for large systems development....” The second version of the system included multiple version handling, aka dependency management. The company created this system because they needed it, and as a result it did improve their productivity. Given this early example we can define a software engineering system as a system which is developed and used by a software development organization to improve their software engineering productivity with the goal of cutting costs and time to market. I worked with, and on, LinkedIn’s software engineering system known as Multiproduct. Microsoft [6], Google [7,14], Facebook [3], and Netflix [10] have also talked about their software engineering systems at conferences. Each organization has built their own software engineering system, has created a dedicated team to maintain and enhance the software engineering system, and has built a system that meets the unique needs of that organization. Each instance of the software engineering system has the same goal to improve the productivity of the software engineering organization to save the company money and make more money faster. With one notable exception of Microsoft (Azure DevOps), these companies have not been selling their system to make money directly, they created these systems to support other money-making activities. The authors of Accelerate [9] draw a direct inference from software productivity to company success in business, that is to say effective software engineering systems drive more profits. Accelerate documents technical capabilities that drive software development performance. They describe what a software engineering system needs to do. However, they do not prescribe the “right” or “best” software engineering system to use. The reason for this is that their research found high performance software engineering organizations, and low performance software engineering organizations across many different industries, using many different engineering systems.  What all high performers had in common is that they shipped code to production frequently, many times per day, implemented changes in less than an hour, were able to resolve production problems in less than an hour, and failed infrequently less than 15% of the time. The high performers were not using the “one correct software engineering system”, they were using their software engineering system which worked well for them, and which made them high performers. Saying this a different way, Google’s engineering system coevolved with Google and is optimized for Google’s particular needs, applications, and culture, same for Microsoft, Facebook, Amazon, LinkedIn and so on.  Descriptions of engineering system goals tend to range from very business focused, like “The goal of this system was to save the company as much product development money and lead times as possible.” from ICL (International Computers Limited) [13] to software engineering best practice focused like this example form MSFT (Microsoft Corporation) One ES (Engineering System) [6]: Some engineering systems are described based on their current implementation, like this example from Google [17]: Or this example from Facebook that is focused on just the SCM aspect of the engineering system [5]: LinkedIn’s Multiproduct shared some of the goals mentioned already, such as collective code ownership, shared components, common toolchain. Many of these goals are implementation details in practice, results of the engineering system’s evolution within the organizations that they serve and were developed in. These systems were created to help the engineering organization become more effective, more productive, and to continue to scale. The real goal of an engineering system then is closer to the business goal explicitly stated by ICL: work faster and spend less money. The research published in Accelerate confirms that companies whose engineering systems have lower lead times result in better overall organizational performance. They identify a set of metrics that measure the performance of a software engineering system: Keeping the definition of the software engineering system in mind along with the state goals for this system let’s explore two dimensions of these systems, repository granularity and code sharing. One of the technical practices that must be supported by a modern engineering system is Continuous Delivery which requires that the engineering system provide version control. Source Code Management systems exist to facilitate version control at different granularities. The discussion around repository granularity is often framed as a choice between a Monorepo and not a Monorepo [2,9]. In practice different engineering organizations store code at different granularities based on the criteria that makes sense within their organizational structure.  Google [14] decided early in the history of the company to keep all the code in one repository, this is what I will consider a Monorepo. The granularity of this repository is the entire code base of the company (except for Android, and Chrome). Facebook [5] has made a similar commitment and has decided to keep all the company's code in the same repository. Both companies have made a significant investment in custom source code management implementation. Google implemented Piper, and Facebook implemented fbsource based on Mercurial. These were significant infrastructural investments which allowed both companies to continue scaling to support tens of thousands of engineers all constantly committing to a single common huge repository. Microsoft is an example of a company that stores all the code from the same organization in one repository. The most famous repository is of course the flagship product of Windows [6] which also happens to be an organization within the Microsoft company. There are other smaller organization level repositories within Microsoft. The Windows repository stands out for its size which made it impossible to effectively use the off the shelf version of git and has forced Microsoft to implement the Git Virtual File system to allow the git client to work with the multi-Gigabit source code repository. Some companies like Pinterest, Stripe, and Uber [1] have opted to organize all the code written in the same language in one language level repository. Pinterest has a repository for Java, Python, and Go, Uber similarly has all its Go code in the same repository. These companies, and these repositories are on a much smaller scale then the Organization and Company size repositories and can leverage off the shelf technology to manage all the source code. Uber has opted to store all the Android applications and library code in a single repository; this is also a choice they made for their iOS code base. Uber has all their iOS code in one repository and all their Android code in a different repository. These repositories tend to be smaller in scale than language repositories with fewer independent artifacts since they are primarily focused on managing application code and Uber has fewer iOS and Android applications than services written in Go. Platform granularity repositories face interesting challenges when it comes to integrating with the platform IDEs (Integrated Development Environments). XCode is the only officially supported IDE tool for iOS development, and it does not scale well to working with large code bases. Uber has had to implement tooling to allow iOS developers to continue to use XCode with their iOS wide code repository. For Android, Uber has chosen to use IntelliJ instead of Android Studio because Android Studio did not scale to handle their Android Code repository well. Open-source frameworks like React and Ember have opted to keep most of their code in a single repository even though these frameworks are modular and can be consumed at a more granular level.  Engineering Systems at companies like LinkedIn and Netflix [11] implemented code repositories at the level of a project. Some of these repositories contain code for a whole microservice or an application. This unit of granularity is associated with the release of the projects and often contains one deployable artifact. Examples would be a gRPC service or a Spark job. This is also the granularity of most public GitHub repositories. This is the lowest level of repository. These repositories contain a single library or a single executable tool. These repositories tend to be tiny and self-contained. Plenty of these exist in the open-source JavaScript ecosystem.  Version control is one of the capabilities necessary for the engineering system to enable Continuous Delivery. The granularity of the repositories creates constraints on the other aspects of Continuous Delivery. The automated build and test system must work at the scale of the repository and their implementations will make different tradeoffs based on the size of the repository. This also applies to the developer tools like IDEs (Integrated Development Environments) and command line tools developers use to interact with the code base.  Code sharing is the core capability of an engineering system. To facilitate sharing code an engineering system needs to provide a mechanism for discovering existing code, integrating existing code into new applications, and keeping shared code up to date in all the applications that integrate it. There are multiple different implementations of code sharing in existing engineering systems. Discovery is facilitated with a code search capability, depending on the cumulative amount of source code managed by a company it can be as simple as code search in the IDE for code bases, under a million lines of code, to stand alone search engines for huge code repositories, billions of lines of code. Each open-source language ecosystem provides a search engine to find existing shared code based on code base metadata like project name or project description. Dedicated code search engines like the one used in Google [14] and LinkedIn allow developers to search actual source code using method and object names, as well as code patterns. Some engineering systems implement code integration by compiling common source code into different applications, while other engineering systems implement code integration by including pre-built packages in different applications and compiling application code independently of shared code. It is common to share source code within a code repository, while sharing packages across repositories, however the code repository structure does not dictate how code must be shared. Google [14], which uses a companywide code repository, implements source code sharing. Netflix [11], which uses project code repositories, implements packaged code sharing. Both engineering systems provide the ability to specify which code needs to be integrated into the applications and a build system which supports granular dependency declaration. Shared code updates are delivered to applications based on how code integration is implemented. In practice these two capabilities are tightly coupled, although they do not necessarily need to be, and are often aligned with code management strategy. For company-wide code repositories that implement source code sharing, updates are delivered to applications during the build process of the shared source code which results in a new build of the application ready to deploy. If the application fails to build the shared source code change is rejected from the company wide repository. For engineering systems that implement packaged code sharing the update process is implemented in multiple steps, first a new package is built, and subsequently every application which integrates this package needs to be rebuilt independently and made ready to deploy. If some applications fail to be built with the updated package the ecosystem potentially has multiple versions of the shared package code creating a challenge for application developers who need to figure out which one to integrate into their applications. Shared code updates are supported by the practice of automated testing which is essential for continuous integration and delivery. Shared code updates need to be compatible with their prior versions to enable fast integration into applications. Engineering systems implement compatibility testing to support shared code developers. The current state of practice in compatibility testing is executing the automated tests implemented in the application code with the updated version of the shared source or package code. This practice can be improved with better granularity of test selection based on the actual call graphs in the application code. The limit of compatibility testing is that it can demonstrate when a change is not compatible, it cannot prove that the change is compatible. The current state of research in compatibility testing is focused on test input generation based on real world call patterns which will improve on finding incompatibilities, in proving the true positive rate, while reducing the noise, the false positive rate. The compatibility constraint is non-trivial and requires significant effort from the maintainers of shared code. The more applications use shared code the more difficult it becomes to meet the compatibility constraint. Maintainers of shared code need tools to support their efforts, they need the ability to know about all the applications that use shared code as well as how the shared code is being used. Compatibility testing tools are necessary to help maintainers make compatible changes. Additionally, tooling is necessary to enable maintainers to roll out incompatible changes which require code changes in the applications that use the shared code [18]. Finally, code needs to be designed for sharing, which includes designing clear public APIs (Application Programming Interfaces), minimizing necessary downstream dependencies, and clear upgrade processes. Lack of effective tooling often results in maintainers finding workarounds for compatibility constraints, for example increasing the Major version when Semantic Versioning is being used for packaged code sharing or copying code into a new namespace when using source code sharing. Incompatible changes often take longer to update and as a result require that multiple versions of the same code co-exist in the ecosystem until all applications have been updated. It’s worth examining how existing software engineering systems choose the repository granularity and code sharing approach. Google’s software engineering system has been described in several publications [7,9,14,18] and conference presentations [17]. Additionally Google recently published a book, Software Engineering at Google, which goes into the details of the design choices they’ve made. Google chose to implement their engineering system around a company-wide code repository and a philosophy of having a single tool for all engineers at Google to use. Google’s engineering system implements trunk-based development; however, code is released from branches. Over time Google has invested in building custom implementations of the code repository, Piper, which is implemented on Google’s custom distributed datastore Spanner (A globally distributed synchronously replicated database system developed by Google). Google built a custom build tool which is optimized to work with their company-wide code repository. The key to this build tool’s efficiency is a granular dependency definition system which allows the build tool to know the projects that need to be rebuilt when a change is made to the repository (Bazel is the open source version of this tool). Google’s engineering system implemented source code sharing; the source code is compiled into the binaries that share the source code. Discovery is supported by a custom code search engine which is praised by every Google engineer I have met. As Google’s repository grew it became impossible to check out the code to a developer machine, to address this issue Google implemented a virtual file system and enabled granular code checkout capabilities in their code repository, allowing developers to checkout only the code they need while supporting rebuilding all the affected projects. Google has made additional investments in tools for making code changes across the entire code repository safely. The engineering system also takes advantage of the centralized code review process to integrate automated checks into each code review, which helps engineers find bugs, and measure regressions in binary size. Google consumes open-source code the same way it consumes it is on proprietary code, via source code sharing. Open-source code is checked into the company repository under the third-party directory and implements integration with their internal build tooling and dependency management system. Google has open sourced their build tool as Bazel, however it has not open sourced its code repository implementation or other auxiliary tools. More recently Google has been investing in a browser-based IDE to work with their code repository. It might be useful to consider the context in which Google started todevelop its engineering system. Google started out with a sole product; a search engine implemented in C++. It makes sense to keep all the code in a single repository when you have a single binary to build and ship. The open-source ecosystem did not exist as it does today with centralized package management or developer platforms. C++ open source to this day implements source code sharing. Facebook’s engineering system has been described in several publications [3,15,16] and conference presentations [5]. Like Google, Facebook has developed a single company wide code repository. Over time they have made investments that are like Google's investments, implementing a custom source code repository, built on top of Mercurial. They implemented a custom build tool to work with the company wide repository, Buck, which has been open sourced. Facebook too had been working on a custom web browser-based IDE, although recently they have decided to leverage Visual Studio Code platforms to develop their IDE. Facebook’s engineering system uses trunk-based development; however, software is released from branches. Like Google, Facebook implements source code sharing. Facebook started out as a single monolithic web application built with PHP, now Hack, a dialect of PHP that Facebook has developed and subsequently open sourced. One product, stored in one repository, deployed as one binary. Netflix’s engineering system has been described in several conference presentations [10,11]. Unlike Google and Facebook, Netflix uses project level code repositories and implements packaged code sharing. Netflix engineering system uses an open-source build tool, Gradle, to facilitate both dependency management and building binaries. Netflix has contributed to the open-source build tooling with Gradle plugins, some of those changes have influenced the direction of the core Grade tool developer roadmap. Netflix practices trunk-based development. Netflix uses commercially available source code management, GitHub, to host their code repositories. Over time Netflix has introduced new build tools into their environment to support different programming languages and dependency ecosystems, for example using npm for the JavaScript web development ecosystem. Netflix has a philosophy of personal responsibility; each engineering team is responsible for their choices. The Netflix engineering system provides a paved road for software development; it does not mandate that everyone stays on that road. Netflix evolved from a Java monolith during a time when open-source tooling around the Java ecosystem was growing and gaining adoption. Netflix invested in service-based infrastructure and split up their initial monolith which resulted in them managing multiple code repositories. The Netflix engineering system evolution is like LinkedIn’s engineering system evolution and shares some of the similar concerns. Over time Netflix has invested in dependency management automation and conducted early experiments with distributed refactoring tools. Netflix operates a distributed eventually consistent engineering system. Multiproduct was developed as a response to a significant drop in productivity of the LinkedIn Software Engineering Organization after years of rapid growth of both the LinkedIn product and Software Engineering Organization. Like Netflix, LinkedIn uses project level code repositories and implements packaged code sharing. LinkedIn’s engineering system also started out with using Gradle as the one build tool which supported polyglot development. Over time Gradle was extended to support development of Python, C++, JavaScript, iOS, and Golang projects. LinkedIn practices trunk-based development, and uses commercially available source code management tools, GitHub, Gerrit & Review Board. Over time new tools have been introduced into the Multiproduct engineering system with different levels of success. Yarn is used to build JavaScript. Bazel is the build tool for iOS and some C++ code bases. Like Netflix, LinkedIn invested in service-based infrastructure and split up not only the original monolith applications but additionally the original monolithic code repository. LinkedIn was ahead of the industry, and influenced organizations like Netflix and others. BitMover[12] is an interesting example of an engineering system which was built around the idea that source code management is the most important aspect of developer productivity. BitMover created a source code management system BitKeeper, and sold it as a product that explicitly depended on the ability to have many branches which are easy to keep coordinated with each other as a mechanism to enable concurrent development. This was how BitMover worked internally and how it encouraged its clients to work. Each of the engineering systems described here evolved in the unique context of the company which uses them. There is a tight feedback loop between the engineering system and the company it serves. Attempting to adopt a company's engineering system wholesale without understanding the context in which the system was developed will lead to failure or will be very painful to do. OK, that was a lot. Let’s say that you are trying to implement or evolve the software engineering system your organization is using, how would you know if the changes you are making are improving the system?  The research documented in Accelerate [8] outlines four measurements which can be used to identify high performance teams and differentiate them from medium and low performance teams: These metrics measure software delivery performance optimizing which is part of the goal for an engineering system. Accelerate differentiates software delivery from software development since measuring performance in software development is particularly tricky. Lead time is the time from code committed to code being used in production. It can be expanded to measure the development process as well. One way to extend lead time is to start the clock when the developer creates a PR or submits code for a review. A further extension will be to start measuring lead time from the moment work starts on a code change, from the first local commit, or even the first change made to a file. With each step, more of the engineering system is being captured by the lead time metric. Building on the work in Accelerate, researchers at Microsoft [4] suggest that other aspects of the engineering system need to be instrumented. For example, the sentiment of developers using the engineering system since there is a connection between the quality of tools and engineers feeling productive. Additional metrics are suggested for measuring the different steps in the engineering pipeline, from development to code review, and of course software delivery. Based on the goals for the engineering system to work faster and spend less money measuring the latency, aka lead time, of the engineering system, and its throughput will be good high-level measures of engineering systems success. Additional granular measures can be introduced to identify and troubleshoot parts of the engineering system that might be impacting latency or throughput. An engineering system can be considered successful when throughput increases linearly with the size of the engineering team while latency remains constant and low, say 1 day from start of implementation to code running in production. I hope you found this article helpful in defining what a software engineering system is and that it gave you the vocabulary to start talking about all the tools, practices, and processes your company uses to build software. If you have questions about this topic please use the comments section, or send me a message on LinkedIn.  Thank you to the many LinkedIn colleagues who provided feedback on this article while I was still at LinkedIn, particularly a huge thank you to Dave Herman who always inspired folks to do great work and was ready to help get you there. 
        To view or add a comment, sign in
 Stay updated on your professional world 
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
        New to LinkedIn? Join now
 