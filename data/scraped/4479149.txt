AI, or artificial intelligence, is technology that attempts to simulate human cognitive function. AI has made its way into the software development space in a number of ways. Visit the AI article list to expand your AI knowledge. Learn More Observability is a way for development teams in an organization to view their program state. Failing to provide developers with insight into their tools and processes could lead to unaddressed bugs and even system failures. Read about the latest observability content here Learn More In the past, the CI/CD pipeline was simply a place to integrate code. Developers would write their code in GitHub, pass it through the pipeline, and then deploy it. The pipeline has become a much more critical piece of the software delivery lifecycle today. Learn More Modern cloud-native applications, often leverage microservices, containers, APIs, infrastructure-as-code and more to enable speed in app development and deployment Learn More Sponsored By: Always On, Always Fast Deploy Distributed PostgreSQL in just 90 Seconds Learn More  The Importance of Security Testing With more development teams today using open-source and third-party components to build out their applications, the biggest area of concern for security teams has become the API. This is where vulnerabilities are likely to arise, as keeping on top of updating those interfaces has lagged. Read the Guide Here Mobile App Testing involves analyzing mobile apps for functionality, usability, visual appeal, and consistency across multiple mobile devices. It helps ensure an optimal user experience, irrespective of the device used to access the app. Learn More Today’s distributed software environments incorporate a variety of APIs with every interface your software touches, from mobile to microservices. Each API has to be continuously tested and verified to ensure your software functions as it should. Parasoft’s API testing platform makes quick, efficient, and intelligent work of such requirements. Learn More Ensure your application’s resilience, and make sure your software performs as expected under diverse operating conditions. (sponsored by Parasoft) Learn More    DevSecOps is the DevOps community’s approach to bringing security into the development lifecycle. Businesses want to deliver software, but cannot afford to release unreliable or insecure applications— therefore security needs to be baked in much sooner than it has traditionally been. Learn More Securing an application is just as important as building it in the first place. As data becomes more valuable, there are more people who want to steal it and use it for their own personal gain. Making sure applications are indeed secure has always been a challenge, as hackers try to stay one step ahead of defenders. Learn More Secrets are essential for integrating your infrastructure with databases and SaaS services. Doppler‘s developer-first security platform empowers teams to manage, orchestrate, and govern secrets across any environment. In 2023, there was an 18% decline in the number of open-source projects that are considered to be “actively maintained.” This is according to Sonatype’s Annual State of the Software Supply Chain Report. Learn More Development Managers need a different type of content than developers… They need to know what platforms, tools, trends, and issues they should be thinking about. SD Times delivers those unique topics here Learn More Agile software development has been around since the 1990s, but didn’t get the name until the famous meeting of 17 renowned software development thought leaders at Snowbird, Utah resulted in an Agile Manifesto. The idea behind Agile software development is to reduce time to market by enabling faster iterations of smaller segments of software. Learn More Value stream management involves people in the organization to examine workflows and other processes to ensure they are deriving the maximum value from their efforts while eliminating waste — of resources, time and assets. It is the practice that truly brings the business side and IT side together as partners in creating value for the organization. Learn More Learn More DevOps is a methodology in the software development and IT industry. Used as a set of practices and tools, DevOps integrates and automates the work of software development and IT operations as a means for improving and shortening the systems development life cycle. Learn More Gravitee helps organizations manage and secure their entire API lifecycle with solutions for API design, management, security, productization, real-time observability, and more.  AI, or artificial intelligence, is technology that attempts to simulate human cognitive function. AI has made its way into the software development space in a number of ways. Visit the AI article list to expand your AI knowledge. Learn More Value stream management involves people in the organization to examine workflows and other processes to ensure they are deriving the maximum value from their efforts while eliminating waste — of resources, time and assets. It is the practice that truly brings the business side and IT side together as partners in creating value for the organization. Learn More Service virtualization has helped countless organizations perform tests on application components that live outside their development organizations, or that are not available to the tester when needed to complete their tests. Virtualization enables organizations to put up a virtual service more easily than they can “yank a box on an Amazon server,” explained Shamim Ahmed, DevOps CTO and evangelist at Broadcom. Yet today, service virtualization (SV) can be seen as a life cycle technology, empowering what Ahmed calls continuous virtualization. This, he said, “enables even developers doing parallel development right now, just for testing. That’s on the left-hand side. And on the right-hand side, we’ve seen extremes, like customers using service virtualization for chaos testing.” SV helped early-adopting organizations to decouple teams, said Diego Lo Giudice, vice president and principal analyst at Forrester, so that you could decouple customer with client. But, he noted, “with organizations being broken up into small teams, and parallelizing, the work with Agile became very hard. Project managers thought they could manage that. And there’s no way you can really manage a bunch of small agile teams working; making sure that you synchronize them through project management is impossible. And so service virtualization was kind of used a bit to decouple, at least from the testing perspective.” Virtualization enables organizations to put up a virtual service more easily than they can “yank a box on an Amazon server,” Ahmed explained.  So, where is service virtualization being used beyond testing?
 Diego Lo Giudice, vice president and principal analyst at Forrester, said SV remains mainly a testing capability, though he said he is seeing an accelerated use of SV in the API world. “I haven’t really gotten, you know, beyond the typical use cases of testing unreachable or expensive third-party resources,” he said, noting that the biggest use case he keeps seeing is virtualizing mainframe environments. “I love the example a CEO gave me that he was saving a lot of money with service virtualization simply because one of his teams, for testing purposes, couldn’t access the mainframe. They only had a window of 30 minutes a month, and they had to wait every time for those 30 minutes. With service virtualization, they were able to virtualize that access to the mainframe, and therefore the team now kind of had the virtual access to the mainframe available all the time.” Using service virtualization with APIs, Lo Giudice said, is “just one of the types of testing that needs to be done; integration tests, that activity that can be automated, software delivery pipelines. I see it a lot there.” Among other areas where service virtualization is being seen is to create employee onboarding environments. Alaska Airlines uses Parasoft’s virtualization solution for its training, according to Ryan Papineau, a senior software engineer at the airline. With virtualization, he said, “we’re able to scale the amount of people that we have go through our training program.” While there are typically no test cases, Alaska can use the environment to see if the users can perform certain tasks, but none of that gets recorded or impacts the production environment.  But perhaps the biggest area of SV growth is in the test data management (TDM) testing space – a term that Papineau said is “kind of messy, because it can mean a lot of things.” It has become, in a word or two, a catch-all buzzword. “We’ve been screening some new automation engineers, and they’ll put test data management on their resume. But you’ll never see any concept of any tools or techniques listed,” Papineau said. “What I believe that to be is they’re listing it, to say ‘Hey, I use data-driven tests and had Excel,’ and I’m like, that’s not what I’m looking for. I’m looking for data structures and relationships and databases. And that life cycle of creation to modification to deletion. And using an ETL tool, or custom scripts, which we use separately.”  Papineau said that Parasoft’s solution essentially uses data and iterates it over APIs, records it and creates the relationships with the data. Papineau said, “You get this nice exploded, fancy UI that has all the relationships and you can drill down and do cloning and subsetting, so it has a lot of the old traditional test data management aspects to it, but all within their context.”  Broadcom’s Ahmed added that his company, which acquired the Lisa SV software developed by iTKO through its purchase of CA, is seeing much more synergy between servers, virtualization and test data management. “When we acquired Lisa, TDM was not that big. But now with all this GDPR, and all the other regulations around data privacy, TDM is really hard. And it’s one of the biggest problems the customers are grappling with.” Ahmed believes SV and TDM go hand-in-glove. “The way they work together, I think, is another key evolution of how the use of service virtualization has evolved,” he said. “Using SV is actually one of the easier ways to do test data management. Because, you know, you can actually record the test data by recording the back and forth between a client and a server. So that gives you an opportunity to create lightweight data, as opposed to using the more traditional test data mechanisms, particularly so for API-based systems.” He noted that the use of SV reduces “the tedium burden,” because creating the test data for a live application versus creating the test data for an emulator is a much lower amount of TDM burden for the testers and everybody else.” While much about service virtualization has gone unchanged over the last years, much has changed, according to Lo Giudice. Developers are choosing open source more, deciding they don’t need all the sophistication vendors are providing.  “I’ve got data that shows the adoption of service virtualization has never really gone over 20%,” he said. “When you ask developers and testers, what is it that you’re automating around in 2022, I think the system integrators” are the only ones for whom this is key.  “It’s actually very useful” in integration projects, Lo Giudice said. “If you think about Lloyds Banking, a customer that’s got a complex landscape of apps, and you’re doing integration work with good partnerships going on,” service virtualization can be quite beneficial. “If you’ve got an app and it interfaces another 10 big apps, you’d better use service virtualization to automate that integration,” he said. Integration projects between assets held on-premises and those residing in the cloud caused some hardships for Alaska Airlines, Papineau said. The problem, he said, stemmed from internal permissions and controls into the cloud. One of their developers was taking older data repository methods and deploying the cloud, and struggled with the internal permissions between on-prem and the cloud.” Papineau said organizations have to understand their firewalls and the access to servers. “Are your server and client both in local? Are they both in cloud order, and does one have to transverse between the other,” Papineau said. “So what we did there is we stumbled on getting the firewall rules exposed, because now all of these different clients are trying to talk to this virtual server. And so it’s like, ‘Oh, you got this one going up. Now you need to do another firewall request for this one?’ And I am not kidding you. When we did the Virgin (Atlantic) acquisition, viral requests were the largest nightmare in the longest time. So that’s why it’s an internal problem we struggled with and just gave up on it like, No, this is just taking too much time. This should not be this hard. This literally is a firewall overhead problem that we ran into.”. Virtualization is not something you do before you do testing any longer. From the time you start to do your backlog and your design, you have to think about what services you need, and how you design them correctly. Then, according to Broadcom’s DevOps CTO and evangelist Shamim Ahmed, you have to think about how to evolve those services. “We think of service virtualization evolving and on the continuum,” he said. “You start with something simple we call a synthetic virtual service that can be created very easily – not using the traditional record-response mechanism.” He noted that the old way of creating a virtual service relied on the fact that the endpoint already exists. That’s what enabled record and replay,  but in today’s development environment, the endpoint may not exist – all you might have is an API specification, and you might not even know whether the API has been implemented or not. “You need to have new ways of creating a virtual service, a very simple, lightweight service that can be created for something like a Swagger definition of an API. Developers need that when they’re doing unit testing, for example. The way we look at this is what we call progressive virtualization – that simple thing that we created can now evolve, as you move your application from left to right in the CI/CD life cycle.” He offered the example once that application gets to the stage of integration testing, you perhaps need to enhance that synthetic virtual service with some more behavior. So more data is added, and then when you get to system testing, you need to replace that synthetic virtual service with the real recording, so it becomes progressively realistic as you go from left to right.  “There’s a whole life cycle that we need to think about around continuous virtualization that talks about the kind of virtual servers needed to do integration testing, or build verification,” Ahmed said. “And of course, all the other kinds of tests – functional, performance and even security testing – virtual services are just as applicable for those things…  because if you think about the number of third-party systems that a typical application accesses in this API-driven world, you simply can’t run many of your tests end-to-end without running into some kind of external dependency that you do not control, from the perspective of functional, performance and security testing. So you can start to emulate all of those characteristics in a virtual service.” 
Broadcom, Parasoft  
                            David Rubinstein is editor-in-chief of SD Times.                         Ready to subscribe to SD Times? It's just a click away! Subscribe