vege - Fotolia Enterprises want to consolidate data storage, extend its useful life and control costs. But what we often see are silos of storage tied to specific applications, workflows and suppliers. These systems may perform well, but they are not always the most efficient or flexible. Software-defined storage (SDS) is an increasingly viable alternative that can bring numerous efficiencies and ways to cut costs. In this article, the first article in a two-part series, we look at the definition of software-defined storage and the key variants we find in the marketplace. Software-defined storage separates the software that carries out core storage operations from physical hardware. “It is storage controller software that is abstracted from the underlying hardware, so it can run on any hardware, any hypervisor, or on any cloud,” says Gartner analyst Chandra Mukhyala. Typically, software-defined storage operates on x86-based servers, and turns those servers into storage devices. The hardware can be a standard server with its own direct-attached storage (DAS), hyper-converged infrastructure (HCI), or equipment optimised for storage such as a server with a larger than usual number of drive bays. In practice, the type of hardware does not, or should not, matter. SDS software will recognise the capacity deployed regardless of the platform. And software-defined storage can bring together separate physical systems into one storage pool, even if they come from different suppliers. Storage software can run directly on the server’s operating system, in a virtual machine or in the cloud. “The whole point of software device storage is that it is not tied to any particular supplier hardware,” says Gartner’s Mukhyala. “It provides the flexibility to the customer to choose underlying hardware. We don’t want any hardware lock-ins.” Software-defined storage offers two main advantages to enterprises. These are the ability to pool storage, and to swap out hardware independently of the storage software supplier. There are also advantages in storage management and better capacity utilisation. This way, an organisation can create one or more larger storage pools visible across its servers, virtual machines and applications. As the SDS effectively “appears” as regular storage to an application, IT managers can fine-tune physical storage systems without it affecting workflows and operations. This allows for easier upgrades, such as a move to a more powerful controller or a faster version of flash. Potentially, software-defined storage allows buyers to pick best-of-breed suppliers for their applications with higher-performance hardware where it matters and slower generic units for less critical applications. Alternatively, firms can continue to use older and less performant storage in the pool, with SDS allocating it to the most appropriate tasks. This removes the need for buyers to specify more powerful systems across the board just to meet the demands of a few applications. As Gartner’s Mukhyala points out, storage suppliers often sell their arrays in “T-shirt sizes”, with small, medium or large that determine not just capacity, but IOPS too. Without software-defined storage, a move to a higher-performance tier means rip and replace for the entire array, even if it hasn’t reached capacity. All this saves cost. But CIOs also look at software-defined storage because they want a global namespace to pool capacity across locations. And it eases the move to hybrid environments in which storage on-premise, in the cloud and potentially in edge applications forms a common pool. SDS also makes it easier to “burst” to capacity in the public cloud as workloads demand. Software-defined storage also makes it easier to share physical storage across file systems. SDS works well where organisations need to operate different types of file system. The technology should be equally able to handle block storage – for databases, for example – and file and object. Under SDS, file and object are often combined. Software-defined storage also has disadvantages. It can add to rather than reduce complexity. It can present a consistent storage interface to applications, but IT teams still face differences between suppliers’ hardware capabilities. Software-defined storage might not be able to control an array’s more advanced features, so storage managers might need to revert to supplier-specific configuration tools and dashboards. SDS might even perform less well than a supplier’s own storage controllers, while supplier-neutral hardware can offer lower capacities than proprietary systems. Nor are all software-defined storage systems as open as they first appear. Some hardware suppliers sell SDS, but only ship it on a hardware appliance. Others support multiple suppliers’ equipment, but with a limited number of certified builds. And some maintain that a proprietary storage stack is the way to go, and their hardware will not integrate at all with a software-defined storage system. There can also be performance drawbacks to software-defined storage. As the storage software is abstracted from hardware, it is not always able to access supplier-specific features. Proprietary suppliers can access low-level functionality in the storage media to improve speeds or flash durability, notes Mukhyala. “I don’t think there will ever be a point where it will be only software-defined storage. There are some advantages by having an integrated hardware stack,” he says. Nor are there – as yet – any industry standards for SDS.  In part, the lack of industry standards for software-defined storage reflects the variety of ways it can be delivered as well as suppliers’ differing marketing approaches. The purest form of software-defined storage is as software only, with users able to choose any x86 hardware. Then there are suppliers that sell SDS on its own, but also pre-configured on hardware, usually an appliance. Some suppliers sell appliances under their own name, others have partnerships with hardware companies. And some SDS suppliers support multiple hardware suppliers with either preconfigured or reference systems, but stop short of supporting all manufacturers. Some suppliers deploy software-defined storage, but on their own hardware stacks. This aims to blend the benefits of SDS’s abstraction from hardware with the features, control and performance of a single-supplier system. IT buyers should also consider whether SDS will work on bare metal, or via a hypervisor or virtual machine. Increasingly, container support is important too. Although it is a different use case from general storage, compatibility with Kubernetes is another reason to consider SDS. Enterprises, though, typically still choose storage technology according to workloads and the features they need, such as a global namespace or cyber protection. Software-defined storage, then, becomes one way to meet those goals. “Do they want an integrated hardware appliance, or do they want a software-based offering that can run anywhere?” asks Mukhyala. “It is really a deployment choice question, but an important one.”   The next U.S. president will set the tone on tech issues such as AI regulation, data privacy and climate tech. This guide breaks ... A challenge companies are facing while preparing for compliance with climate risk reporting rules is a lack of consistency among ... Key leadership decisions like poor architecture to rushed processes can lead to technical debt that will affect a company ... Data is one of the most important assets in any organization. To truly protect it, you need a DDR strategy. Here's what you need ... Does your organization need every cloud security platform and service currently in use? Tool consolidation can reduce the chances... Following several high-profile attacks across the globe on MFA-less accounts, Microsoft will make the security measure mandatory ... Cisco cuts its workforce by 7% and forms one unit for networking, security and collaboration to energize AI and security sales. ... OWC transfers data using highly directional light in free space. While OWC delivers high-speed data transfers, it is susceptible ... Network architects face challenges when considering a network upgrade, but enterprises can keep problems to a minimum by ... AMD plans to acquire AI systems designer and manufacturer ZT Systems for $5 billion. AMD CEO Lisa Su said hyperscalers want more ... Data center modernization offers a competitive advantage to organizations, along with maximizing hyperscale infrastructure. ... Configuration files are vital for system deployment and management. Consider improving file management with proper planning, ... Pairing retrieval-augmented generation with an LLM helps improve prompts and outputs, democratizing data access and making ... Vector databases excel in different areas of vector searches, including sophisticated text and visual options. Choose the ... Generative AI creates new opportunities for how organizations use data. Strong data governance is necessary to build trust in the... All Rights Reserved, 
Copyright 2000 - 2024, TechTarget


Privacy Policy



Cookie Preferences 



Cookie Preferences 



Do Not Sell or Share My Personal Information
