The technology can translate and summarise texts as well as answer questions, heightening concerns about academic fraud. To test how believable the AI-generated texts are in the eyes of the professionals, a team of scientists in the United States asked their peers to tell research paper abstracts written by the AI writer apart from those by humans. The blind reviews misidentified 32 per cent of generated abstracts as being real and 14 per cent of original abstracts as being written by the chatbot, according to the study published in late December on the bioRxiv website ahead of peer review. “Reviewers indicated that it was surprisingly difficult to differentiate between the two, but that the generated abstracts were vaguer and had a formulaic feel to the writing,” the team from Northwestern University and the University of Chicago said. 01:30 Chess robot breaks child’s finger during an international tournament in Russia Lead author Catherine Gao, from Northwestern University, said she was concerned that AI writing software could hurt the credibility of the scientific community. “Any technology, if misused, could be dangerous,” she said. “What’s unique about ChatGPT is how easily accessible, free, and fast it is. “Organisations such as paper mills [fake research paper factories] could use this technology more easily to fabricate scientific writing and results. If a scientist were to base their work on data that is false, or a clinician to base their care on studies that were fake, that could be very dangerous.” But using the AI service, which is capable of translation and writing in languages including English, Chinese, Spanish and French, could also help scientists publish in a second language, according to Gao. “[The tools] can be used to help scientists with the burden of writing and help improve equity, particularly for scientists who may have language barriers to disseminating their work,” she said. When the Post asked ChatGPT to write a scientific abstract without providing data, on some trials the service generated texts listing the objective and results of a study in the style of medical journals, with a note saying “the results and conclusion may not be accurate”. But in other instances, it said: “I apologise, but as a language model, I don’t have access to primary data or the expertise to conduct a study, so I can’t provide an abstract for a study that does not exist. “It’s important to note that creating an original scientific abstract would require access to primary data, the expertise to conduct a study, and the research team’s permission to share their findings. “Additionally, the study should go through a rigorous peer-review process to ensure accuracy and reliability before being published in a reputable journal.” When asked what it could do for scientists, ChatGPT issued a series of bullet points listing the ways “I can assist scientists”. These included: providing information and answering questions about specific topics in their field of study; generating hypotheses and predictions based on data; summarising and analysing large sets of data; assisting with the writing of research papers and grant proposals; and helping to design and plan experiments and translating texts. Its final bullet point said it could also do “much more”. 01:52 Robot artist Ai-da addresses UK Parliament in historic first The language model also attracted the attention of Guo Yingjian, vice dean of the capital development and governance institute at Renmin University of China, who said in an article in China Science Daily that China’s higher education sector should prepare for its “massive impact”. “Teachers should educate students to take full advantage of this technology to stimulate human creativity and thought,” he wrote. He also raised questions on how its use should be managed, asking whether the use of AI text generators should be treated differently from existing rules on plagiarism. “AI is not yet capable of original academic research and it doesn’t do much for genuine academic writing,” he said. “But with the rapid development of AI, I believe this day is not far away from us.” Wang Yanbo, an associate professor of strategy and innovation at the University of Hong Kong, said he would not worry about AI writing software because human creativity remained key. He pointed to the internet and search engines, which have changed the way students research, analyse and use information, and writing assistants that rule out grammatical errors. “We are entering a new age and should expect every single article to be well written,” he said. With or without the help of AI, ill-intentioned people would “generate fake scientific findings and get them placed in low-quality journals”, Wang said. “For the scientific community, it is about the collective efforts to build a research environment where transparency, honesty and creativity will be honoured and rewarded, with or without the impact of AIs. That’s why data sharing, procedure revealing and replication are so key for scientific research.” Earlier this month, New York city banned the use of the chatbot from its schools’ devices and networks, citing concerns that it does not build critical thinking and problem-solving skills for students. Wang said: “The temptation for students to choose the easy path and let the technology do the dirty work is very high.” But instead of banning it from classrooms, he said teachers would have to redesign how classes were delivered and the way students engaged with the course content. “It is a mandate sooner or later for professors to incorporate such technologies into our course content and into our classrooms so that we can prepare students to actively embrace the new phase of our world,” he said. Gao from Northwestern University pointed to the International Conference on Machine Learning, which has prohibited papers produced entirely by AI writers but says authors can use such tools for editing or refining texts. “It remains to be determined where the academic community will feel the boundaries are for its acceptable use in scientific writing,” she said.