The rise of generative artificial intelligence (AI) has transformed creative problem-solving, necessitating a reassessment of idea evaluation processes to effectively screen and select from the resulting abundance of solutions. This study investigates how human-AI collaboration can enhance early-stage evaluations of innovative solutions, examining the interplay between objective criteria, which are quantifiable and measurable, and subjective criteria, which are based on personal opinion and intuition. We conducted a field experiment in partnership with MIT Solve, a marketplace for social impact innovation. The study involved 72 experts and 156 community screeners evaluating 48 real-world solutions for the 2024 Global Health Equity Challenge, resulting in 3,002 screener-solution pairs. We utilized GPT-4, a state-of-the-art large language model, to provide recommendations and explanations for screening decisions. Our experiment compared a human-only control condition against two AI-assisted treatments: a black box AI providing recommendations without explanations and an explainable AI offering both recommendations and rationales for its decisions. Our findings reveal that screeners strategically use AI insights, validating AI’s recommendations when they agree and scrutinizing AI recommendations when they disagree. Screeners assisted by AI were roughly 9 percentage points more likely to fail a solution than the control condition, primarily influenced by AI’s more stringent failure recommendations. Notably, when AI provided explanations for subjective criteria failures, screeners were 12 percentage points more likely to adhere to these recommendations compared to the black box treatment condition. Moreover, the reported effects were larger among the community than expert screeners. Data from interviews and mouse tracking reveal that AI explanations for subjective criteria led screeners to doubt their own human judgment and possibly over-rely on AI’s explanations. This research suggests a possible framework for human-AI collaboration in creative evaluation, where AI corroborates human judgment on objective criteria and humans retain primary responsibility for subjective assessments supported by AI insights. The rise of generative artificial intelligence (AI) has transformed creative problem-solving, necessitating a reassessment of idea evaluation processes to effectively screen and select from the resulting abundance of solutions. This study investigates how human-AI collaboration can enhance early-stage evaluations of innovative solutions, examining... A century ago, Neyman showed how to evaluate the efficacy of treatment using a randomized experiment under a minimal set of assumptions. This classical repeated sampling framework serves as a basis of routine experimental analyses conducted by today’s scientists across disciplines. In this article, we demonstrate that Neyman’s methodology can also be used to experimentally evaluate the efficacy of individualized treatment rules (ITRs), which are derived by modern causal machine learning (ML) algorithms. In particular, we show how to account for additional uncertainty resulting from a training process based on cross-fitting. The primary advantage of Neyman’s approach is that it can be applied to any ITR regardless of the properties of ML algorithms that are used to derive the ITR. We also show, somewhat surprisingly, that for certain metrics, it is more efficient to conduct this ex-post experimental evaluation of an ITR than to conduct an ex-ante experimental evaluation that randomly assigns some units to the ITR. Our analysis demonstrates that Neyman’s repeated sampling framework is as relevant for causal inference today as it has been since its inception. A century ago, Neyman showed how to evaluate the efficacy of treatment using a randomized experiment under a minimal set of assumptions. This classical repeated sampling framework serves as a basis of routine experimental analyses conducted by today’s scientists across disciplines. In this article, we demonstrate that Neyman’s methodology can also... The case reveals how Porsche has become one of the world’s leading car companies. Central to Porsche’s growth strategy is creating great products, including its legendary 911 Carrera sportscar, and offering innovative customer experiences. As the automotive industry is undergoing disruptive changes, the company’s leadership is making changes to its product portfolio and customer experiences. The case reveals how Porsche has become one of the world’s leading car companies. Central to Porsche’s growth strategy is creating great products, including its legendary 911 Carrera sportscar, and offering innovative customer experiences. As the automotive industry is undergoing disruptive changes, the company’s leadership is making changes to... As the world of operations has changed, so have interests and priorities within the
                  Unit. Historically, the TOM Unit focused on manufacturing and the development of physical
                  products. Over the past several years, we have expanded our research, course development,
                  and course offerings to encompass new issues in information technology, supply chains,
                  and service industries. 
                The field of TOM is concerned with the design, management, and improvement of operating systems and processes. As we seek to understand the challenges confronting firms competing in today's demanding
                  environment, the focus of our work has broadened to include the multiple activities
                  comprising a firm's "operating core": 
                There are no upcoming events. 
                     Technology & Operations Management Unit
                     Harvard Business School
                     Morgan Hall
                     Soldiers Field
                     Boston, MA 02163
tomunit@hbs.edu
