Drew Spaventa is the founder and CEO of The Spaventa Group, a leading investment company.   When OpenAI unveiled ChatGPT to the public, the technology instantly took the world by storm. The artificial intelligence (AI) chatbot gained 1 million users within the first five days of its release and 100 million users by January 2023, sparking a flurry of press activity that crowned the introduction of the large language model (LLM) as everything from “a tipping point” in AI technology to “the most important news event” over the last year. But ChatGPT hasn’t been the only AI tool to make headlines this year. Other programs, like Stable Diffusion—a text-to-image model released by Stability AI and LMU Munich’s CompVis group—have attracted 10 million users since its launch in August 2022. Text-to-voice programs, like Neosapience’s Typecast AI, have also proven to be popular, crossing the 1 million-user threshold early last year. ChatGPT, Stable Diffusion and Typecast are all examples of generative AI—a broad term used to describe technologies that transform natural (human) language prompts into multimedia content such as text, images, video, audio and code. Perhaps an interesting (and admittedly meta) exercise would be to query ChatGPT for a definition of this term. If we ask it, “What is generative artificial intelligence?” here’s how it responds: “Generative Artificial Intelligence (AI) refers to the type of AI that is capable of generating new data or information based on the patterns and structures it has learned from existing data. It is an approach to AI that focuses on creating models that can simulate human creativity and intelligence. Generative AI models are typically built using deep learning techniques, such as neural networks, and they are trained on large datasets of examples. Once trained, these models can generate new data, such as images, music, or text, that is similar in style and structure to the training data.” But how exactly do these generative AI models work? According to Stephen Wolfram, the founder of the mathematical engine WolframAlpha, large language models (LLMs) like ChatGPT are essentially sophisticated forms of auto-complete. Put another way, ChatGPT functions as a predictive model that outputs text with the highest probability of producing a “reasonable continuation” of the input prompt based on its training data. Interestingly, this is a sharp departure from the way humans think about language. For us, language is symbolic—a way for us to encode and communicate with others information derived from our thoughts, feelings, experiences and instincts. Human language is also governed by grammar, a framework that describes how words can be put together to form well-formed—or syntactically correct—sentences. Though neither of these factors are probabilistic in nature, it appears that LLMs are better at grammar than they are at grasping the symbolic power of language. This conclusion is not surprising: Grammar (mostly) follows a closed-form structure, while understanding the meaning behind a sentence (or even more abstract ideas like themes and motifs) is a more daunting task for an LLM whose knowledge of the world is circumscribed by its training set. Natural language processing models like ChatGPT will sometimes generate syntactically correct but semantically incorrect sentences—leading to plausible-sounding but false statements. ChatGPT seems to struggle the most when asked to cite scholarly works, as this Twitter thread by University of Queensland economics professor David Smerdon illustrates. In a January 2023 publication, OpenAI said that it was combatting the model’s tendency to “hallucinate” facts by engaging in reinforcement learning through human feedback (RLHF), a manual process that employs human data labelers to rank outputs and reward higher-quality responses. The model is then fine-tuned against this feedback and redeployed for public use. While the technology is nascent (and highly controversial as it has sparked a newfound interest in the dangers of artificial intelligence), there is already discussion of its use cases for the financial services industry. And although AI and machine learning has been in use for over a decade, these technologies utilize existing data to make predictions, while generative AI creates new content. As such, the major use case in financial services I see generative AI having a profound effect on is the client experience. As with any AI technology, without data, it’s effectively useless, and financial companies maintain troves of data about their customers that only said financial companies would be able to innovate upon. The best example of this is Bloomberg recently launched BloombergGPT, their attempt to make a ChatGPT for the financial services industry. I can foresee financial companies using either a product created by another company or developing their own technology as Bloomberg did, to enhance the client experience by providing clients a simple way of monitoring investment portfolios, getting answers to essential financial questions and assisting investors with simple financial decisions. I can see generative AI being akin to a junior associate or a “financial assistant” to investors and working in tandem with seasoned financial professionals—not entirely replacing them. I think that financial decisions are too emotional and personal for individuals to rely 100% on the efforts of AI without human intervention. That said, the year is 2023 and things can change over the next 20 to 30 years as we continue to evolve as a species and technology continues to advance at a feverish pace. Forbes Finance Council is an invitation-only organization for executives in successful accounting, financial planning and wealth management firms. Do I qualify? 