Generative AI has powerful potential and comes with inherent risks – all leaders should know how it works and what it can offer business 
When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works.
 Businesses have sought different forms of automation and intelligent data processing for years, but generative AI has opened the floodgates of investment and enterprise interest. Formed on the back of machine learning and natural language processing (NLP), which have been in the enterprise wheelhouse for many years, generative AI entered the public eye through the popularity of ChatGPT and has since come to dominate discussions in the tech sector. The relatively new approach to AI is already being used to generate detailed text and image outputs using simple user input and is increasingly integrated within business environments to automate a range of menial tasks.  Generative AI has sparked interest across the business world because of the degree to which it can be personalized. With the right approach, the technology can radically improve worker productivity and help companies provide customers with far more intuitive user experiences. Generative AI’ is a term that refers broadly to AI systems that are capable of producing outputs based on a prompt. These come in a few different forms that work in subtly different ways. The most popular models in use today rely on a complex artificial neural network (ANN) architecture known as a transformer. In simple terms, transformers take a prompt and output a response based on statistics and the exhaustive training process to which they have been subjected. Transformers convert inputs into context, by breaking words down into mathematical values that are used to inform a model’s output. For example, when a user inputs “Where is Microsoft’s HQ?” into a model, the transformer converts the words into ‘tokens’ of data which are combined to form a coherent ‘vector’ of context used to produce a statistically relevant output. Receive our latest news, industry updates, featured resources and more. Sign up today to receive our FREE report on AI cyber crime & security - newly updated for 2024. If the model in the example had been trained well and on a specific company’s data, it could pick out the contextual significant of the words ‘our company’ and ‘city’ to produce a relevant output i.e. “Microsoft is headquartered in Redmond, Washington”. As opposed to the form of AI that attempts to categorise information, generative AI relies on modelling that tries to understand the dataset structure and generate examples that might relate, or match. The two main forms of neural networks at play here – GANs and transformers – work in slightly different ways. By and large the former is involved in creating visual and multimedia content from images and text, while the latter uses information on the internet to generate textual output. Although generative AI became popular in 2022, experts have been working on the technology in theoretical and practical applications for many years. Dutch researchers wrote about the philosophical underpinnings of generative AI as far back as 2012. Indium Software, too, released a white paper [PDF] less than three years ago which highlighted not just how generative AI could be used creatively, but also in high-friction workplaces like healthcare. Transformer models rose to prominence through the research paper Attention Is All You Need and quickly became prized for the efficient and performant way in which they could be used to produce coherent AI output. This was a significant breakthrough, which fed directly into subsequent products at OpenAI, Google DeepMind, and other firms. The generative AI market has grown incredibly quickly, to the extent that there are now innumerable LLMs and tools that rely on the technology.  For many businesses and consumers alike, the standout example will still be OpenAI’s ChatGPT. Microsoft users also now have access to Copilot, the company’s AI productivity assistant, across a wide range of apps in the 365 suite, as well as within Bing search. Google’s Gemini AI family powers the company’s AI product offerings, including Google Cloud’s new Vertex AI Agents. AWS has also invested heavily in AI tools, having rolled out solutions such its chatbot for enterprise Amazon Q. AI pair programmers such as Code Llama, Gemini Code Assist, or GitHub Copilot can provide code suggestions based on a company’s private codebase or make existing cod emore efficient. These tools can also analyze code to provide user-friendly explanations of its function and produce comments, or translate it from one programming language into another. Open-source AI models are also growing in popularity. These include models from AI community Hugging Face, with models in the space rapidly approaching the sophistication of some proprietary models. In recent months, however, experts have questioned whether these models are truly open, as developers continue to impose usage restrictions on some models in their licenses. For example, open models from developers Meta and Databricks come with clauses that prohibit their use by firms with more than 700 million active monthly users, without express permission from the developers. The latest flagship models, including OpenAI’s GPT-4, Google’s Gemini 1.5 Pro, Meta’s Llama 3, and Anthropic’s Claude 3 are multimodal. This means they can process text, images, video, or audio as inputs and produce outputs in a variety of formats. Gemini 1.5 Pro also uses an architecture known as ‘mixture-of-experts’ (MOE), in which multiple ANNs dubbed ‘experts’ are assigned inputs based on which expert will most effectively process the input to produce an output. The MoE approach has been hailed as a cost-effective route for training compared to prohibitively expensive traditional methods. It allows for models to increase in size and effectiveness, especially for multimodal processing, and is likely to become more widely-used in future models.  Discover the leading service providers in enterprise supply chain innovation Part of the reason that generative AI has seen such a boom in popularity is its ease of use relative to its power and potential. Generative AI has opened the door to far more detailed responses to natural language inputs, with LLMs able to unpick meaning from user queries and provide informed responses on its own. Through generative AI-powered chatbots, businesses can provide customers with personalized answers based on questions to improve the overall digital experience of their website. Using generative AI, enterprises can automate manual tasks such as drafting text, collating data from across different sources, or identifying anomalous details in a file or image. Summarization stands out as a major benefit of major AI, particularly when used across a company’s estate. The chief appeal of products such as Microsoft Copilot or Google’s Vertex Agents is that they can provide users with accurate answers to specific questions grounded in their company’s data. For example, a user could ask for a summary of an internal medical policy. AI code generation is another major use case for generative AI. Using an AI pair programmer, developers can speed up their workflows and focus on problems that require expert attention rather than sinking time into simpler work. The capability for these tools to translate code from one language to another has massive applications for legacy codebases, which may be written in obscure languages like COBOL.  Easy code translation could save companies time and money down the line or even prevent critical outages, as experts with firsthand experience of these old languages are harder and harder to find. More recent developments have allowed generative AI models to be used for tasks such as live video analysis through computer vision, which has applications from accessibility in tech to more autonomous robots in a manufacturing environment. Widespread use of generative AI has been matched with concerns over its potential risks and harms. From the earliest models in public use, it has been clear that AI has drawbacks such as ‘hallucinations’ – the term used to describe incorrect statements confidently reported by an LLM. Hallucinations are just one problem with generative AI. Leaders will need to keep up to date on the latest issues and remedies in the space as they seek to integrate it within their tech stacks, not least because of an anticipated increase in regulatory oversight of AI similar to the GDPR. The rapid growth of the generative AI market has led to widespread discussions over the importance of ethical AI. One of the most basic concerns when it comes to generative AI models is who owns the data used to train the models – with some developers already facing lawsuits from artists, writers, and publishing houses over the alleged use of copyrighted material for training LLMs. This is the tip of the iceberg for the legal issues of generative AI and governments around the world are progressing AI legislation to control the risks and harms that AI could pose. The AI Bill of Rights was drafted in the US in 2022, as a framework for shaping future regulation. But the US has fallen behind on AI since, having lagged behind the EU and UK's AI approach. The EU AI Act seeks to regulate AI models – inclusive of generative AI – according to their assessed risk. Businesses will need to know how their AI is being used and have a good understanding of the data used to train the models they use, or face hefty fines.  AI-linked job cuts are a major point of concern for employees and the speed and sophistication of generative AI has fanned anxieties in this space. In 2023, IBM’s CEO Arvind Krishna was forced to go into damage control mode after stating that generative AI would benefit productivity at firms at the expense of human roles. However, Microsoft found that more workers care about AI benefits than its job impacts in its 2023 Work Trend Index, emphasizing how the benefits of generative AI are being increasingly weighed up against its downsides. Job losses due to generative AI are not a given. Leaders can pursue upskilling to prevent AI cuts, insulating workers with AI skills that keep them in their roles longer. Generative AI threats, those specifically linked to attackers misusing generative AI to launch more sophisticated attacks on victims, are a major focal point for security teams as the technology becomes widespread. Though the  One risk of generative AI that’s already been widely discussed is the potential to exacerbate the rise of deepfakes. These images or videos are created in such a way that renders a lifelike imitation of a person, often a celebrity but also potentially a prominent business leader, in such a way that it can trick others. Real-time deepfakes are now becoming a more serious threat, while voice cloning tools such as Microsoft’s VALL-E or OpenAI’s Voice Engine can reproduce realistic copies of people’s voices using less than a minute of sample audio. Both could be effective weapons in a new era of social engineering. At the same time as attackers are figuring out how to use AI to enhance their attacks, AI cyber security is being used by defense teams to counter these threats in more sophisticated ways. Tools such as Microsoft Security Copilot or Gemini in Security Command Center can help identify and summarize threats, or suggest actions for effectively responding to threats. While it’s tempting to see generative AI as a malign force, given the early chaos it has sewn in the creative industries and across parts of the economy, that doesn’t mean it can’t be tamed and channeled into productive use cases. That said, there are many questions that need to be addressed first, with regulation a hot topic at the moment. John Loeppky is a British-Canadian disabled freelance writer based in Regina, Saskatchewan. His work has appeared for the CBC, FiveThirtyEight, Defector, and a multitude of others. John most often writes about disability, sport, media, technology, and art. His goal in life is to have an entertaining obituary to read. AMD celebrates AI advances with partner event 'Local' machine learning promises to cut the cost of AI development in 2024 SolarWinds urges customers to patch critical Web Help Desk flaw 
Posted
 
Posted
 
Posted
 
Posted
 IT Pro is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. ©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036. 