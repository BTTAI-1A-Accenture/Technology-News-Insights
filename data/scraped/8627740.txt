
                Agree & Join LinkedIn
               
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
                Create your free account or sign in to continue your search
                 
              or
             
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
                New to LinkedIn? Join now
 
                  or
                 
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
              New to LinkedIn? Join now
 Greetings, and welcome to the latest edition of my series on object detection using synthetic data from AILiveSim. For those who haven't read the previous articles, I suggest taking a moment to catch up before proceeding. In this edition, I examined the performance of the YOLOv5 model, trained on 100% synthetic data, when applied to real data. This time, I would like to delve deeper into a more thorough analysis of its effectiveness by comparing its performance with a model trained on real data.  This paper suggests that simply training a model on real data is not enough to achieve optimal results. Instead, the study found that having control over the data used to train a model is crucial for achieving superior performance. In this article, I will be comparing the performance of two models - one that was trained on synthetic data from AILiveSim and another that was trained on real data obtained from Roboflow. The comparison will be conducted through two distinct phases - training and testing. This structured approach allows us to not only assess the performance of each model, but also gain insight into the underlying factors that influence their behaviour. To obtain unbiased prediction results, the Singaporean Maritime Dataset was utilised during the testing phase of this study. As a common issue with real datasets available online, most of such resources consist only of images of various ship types. Singaporean Maritime Dataset is one of the most used resources for maritime object detection and even it has only a few instances of buoys and hardly any other floating object. Considering these challenges, the comparative analysis was limited to only two categories, boats and buoys. In order to ensure uniformity and consistency in the findings of this study, I re-categorized several types of ships as belonging to the "boat" class. In this article, three datasets were utilised - two for training object detection models and one for testing purposes. The generation and collection of the synthetic dataset using AILiveSim was previously discussed. A total of 2825 synthetic images were used for training, while 300 each were designated for testing. The real dataset used for training another YOLOv5 model was obtained from Roboflow and originally consisted of two separate datasets for boats and buoys, which were later combined into one complete dataset. To access the relevant datasets, you can follow the links here and here. This real dataset followed the same training and testing split as the synthetic one. Lastly, the two models trained on the above datasets were tested using the Singaporean Maritime Dataset. The Singaporean Maritime Dataset (SMD) is a collection of high definition onshore and on-board videos. You can check out the dataset here. I followed the instructions provided in the Singaporean Maritime Dataset documentation to convert the videos into frames. Moreover, I converted the ground truth MAT files into text files in YOLO format.  In this phase, the models were trained on their respective datasets. It was important to note that the models were trained using the same network architecture and training parameters to ensure a fair comparison. This allowed me to assess the impact of the data on the performance of the model. Moreover, I kept the size of the dataset the same as it greatly influences the performance of a machine learning model. The dataset from Roboflow was in the correct YOLO format and I only had to mix the boat and buoy datasets to form the training and testing split. Below are a few samples from both AILiveSim and Roboflow datasets that were used in the training phase. Images from the Roboflow dataset are displayed on the left and the ones from AILiveSim are on the right side.  The key contrasts between the two datasets are that the AILiveSim dataset is comprised of high-resolution images with a dimension of 1920x1080 pixels, while most of the images in the Roboflow dataset have a size of 416x416 pixels. Additionally, the Roboflow dataset features only one object per image, whereas each image in the AILiveSim dataset has multiple objects. Furthermore, the Roboflow dataset has limited variations, whereas the AILiveSim dataset includes images generated under various weather conditions and sea states. Lastly, the tools in AILiveSim made it possible to tweak the dataset in a way that more closely resembles the Singapore Maritime dataset. This was done by fine-tuning the camera sensors, weather conditions and the sea states in AILiveSim. I would write more about this in another article because this is one of the biggest strengths of simulated data and AILiveSim is also working on automating the process of fine-tuning data to match the needs of their customers. In the final phase, the models were tested on a test dataset to evaluate their performance in a real-world scenario. To evaluate the models' generalisation ability, I used the Singaporean Maritime (SM) Dataset for testing. The test set contains 1880 images. The confusion matrix and the precision curve below represent the performance of the Roboflow model on the Singaporean Maritime dataset. With the maximum F1-score of 0.11, the model was almost unable to generalise on the SM dataset. This is mainly due to the differences in the two datasets. Roboflow dataset includes low quality images that only contain one object per image, whereas the SM dataset contains high resolution images with multiple objects in each image.  The confusion matrix and F1-Confidence Curve presented below showcase the performance of the AILiveSim model on the SM dataset. These results have been achieved through multiple iterations of data improvement by AILiveSim, aiming to make the generated data more closely resemble the SM dataset. The iterative process has allowed for refining the model's performance by enhancing the AILiveSim dataset’s alignment with the SM dataset. These two performance evaluation graphs clearly demonstrate that the model trained on synthetic data outperforms the one trained on Roboflow data in terms of generalisation. A more straightforward method of comparison would be to display the predictions made by each model. The results of the predictions made on the SM test set by both models are displayed below. The images on the left depict the predictions made by the Roboflow model, while the images on the right represent those made by the AILiveSim model. In this comparative analysis, two object detection models were trained, one on synthetic data by AILiveSim and one on real data. The two models were evaluated using a test set to assess their performance and generalisation ability. When tested on the Singaporean Maritime Dataset, the model trained on real data was unable to generalise and showed poor performance, while the model trained on synthetic data performed much better. These findings indicate that the generalisation abilities of each model vary depending on the quality and variety of images fed to it. We saw how a model trained on synthetic data can even outperform models trained on real data. This is because synthetic data can provide a variety of learnable features to a model. Additionally, synthetic data can be fine-tuned and manipulated in a way that is not possible with real data, allowing researchers to tailor the training process to specific use cases. In summary, having autonomy over synthetic data can be even more important than relying solely on real data for model training, but the quality of the synthetic data is crucial for achieving optimal results. Consider following to not miss the next issue: Author: Faiz Muhammad 
        To view or add a comment, sign in
 Stay updated on your professional world 
      By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
     
        New to LinkedIn? Join now
 