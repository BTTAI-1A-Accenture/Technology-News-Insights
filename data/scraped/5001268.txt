LONDON, ENGLAND - JULY 20:  David Florence of Great Britain conpetes with Richard Hounslow at Lee ... [+] Valley White Water Centre on July 20, 2016 in London, England.  (Photo by Bryn Lennon/Getty Images) Data is good. Well, most data used in most scenarios for most purposes is generally good, but we still need to perform data quality assessments upon every data source we use if we are to have any real confidence with it when looking to make use of it in real world enterprise environments. With the rise of generative Artificial Intelligence (gen-AI) never out of the business and technology headlines at the moment, the question of what information we channel into these new Machine Learning (ML) brains that suck on Large Language Models (LLMs) comes into play - especially when so much of that LLM information may reside in a raw a more unstructured format than we might like. Similar concerns are raised when we use real-time data if we fail to provide gain a firm handle on the source of our data provenance, the security status of the data channels we make use of... and the quality and ‘cleanliness’ of our current real-time streams. Keen to guide up the real-time data creek with a paddle, a map and compass and full set of Gore-Tex waterproofs is Confluent, Inc. a company that specializes in data streaming technologies. The company has developed and released new functionalities in its Confluent Cloud offering to give data engineers (i.e. software application development professionals as well as database administrators, systems administrators, higher-level data scientists and others) the ability to know whether or not their data is trustworthy and can be easily processed and securely shared. With Data Quality Rules, an expansion of the company’s Stream Governance suite, organizations can resolve data quality issues so data can be used business-critical and mission-critical decisions. In line with the new products/services that the organization has now brought forward, the team here also notes that Confluent’s new Custom Connectors, Stream Sharing, the Kora Engine and early access program for managed Apache Flink make it easier for companies to gain insights from their data on one platform, reducing (in theory at least) operational burdens when working with data tools. “Real-time data is the lifeblood of every organization, but it’s extremely challenging to manage data coming from different sources in real-time and guarantee that it’s trustworthy,” said Shaun Clowes, chief product officer at Confluent. “As a result, many organizations build a patchwork of solutions plagued with silos and business inefficiencies. Confluent Cloud’s new capabilities fix these issues by providing an easy path to ensuring trusted data can be shared with the right people in the right formats.” Clowes and team point to the benefit of having high-quality data that can be shared between teams, partners and even to some customers depending on the use case. It’s all about feeding applications and services that modern businesspeople will use to make business decisions. If we look inside the IT departments serving these needs, we can see that dealing with highly distributed open source infrastructure like Apache Kafka for data streaming, there are challenges - which is why Confluent exists in the first place to provide a suite of services to overcome the tougher parts of dealing with data streaming. According to claims made by Confluent’s new 2023 Data Streaming Report, some 72% of IT leaders cite the inconsistent use of integration methods and standards as a challenge or major hurdle to their data streaming infrastructure. Aiming to addresses these challenges, the company now details data contracts; these are formal agreements between upstream and downstream components (upstream data is coming from somewhere or some source, downstream data is on its way somewhere to some target application, data repository, service of source) around the structure and semantics of data that is in motion. Confluent says that one particularly critical component of enforcing data contracts is rules or policies that ensure data streams are high-quality, fit for consumption and resilient to schema evolution over time. To build comprehensive data contracts, Confluent’s Data Quality Rules is a feature in its Stream Governance offering. It enables organisations to work with trusted, high-quality data streams across the organization using customizable rules that ensure data integrity and compatibility. “High levels of data quality and trust improves business outcomes and this is especially important for data streaming where analytics, decisions and actions are triggered in real time,” said Stewart Bond, VP of Data Intelligence and Integration Software at IDC. “We found that customer satisfaction benefits the most from high quality data. And, when there is a lack of trust caused by low quality data, operational costs are hit the hardest. Capabilities like Data Quality Rules help organizations ensure data streams can be trusted by validating their integrity and quickly resolving quality issues.” Many organizations have their own custom-built (and custom-tuned and tweaked) data architectures. This means that they need to build their own connectors to integrate their homegrown data systems and custom applications to Apache Kafka. However, these custom-built connectors then need to be self-managed, requiring manual provisioning, upgrading and monitoring, taking away valuable time and resources from other business-critical activities. By expanding Confluent’s Connector ecosystem, Custom Connectors is supposed to allow IT teams to quickly connect to any data system using the organization's own Kafka Connect plugins, all without code changes. It also enables an organization to ensure high availability and performance using logs and metrics to monitor the health of the team’s connectors and workers. Confluent’s new Custom Connectors are available on AWS in select regions. Let's stop and remind ourselves that no organization exists in isolation. For businesses doing activities such as inventory management, deliveries and financial trading, they need to constantly exchange real-time data internally and externally across their ecosystem to make informed decisions, build seamless customer experiences and improve operations. We won’t be leaving the river, canoe, paddle and possibly even waterfall analogies aside anytime soon when it comes to data streaming. The analogy is too much of a perfect fit and its associated metaphors aren’t that far off either i.e. the uncharted waters of the unstructured data lake are close by, remember? Up the creek without a paddle is a fairly universally translated term, so let’s keep a tight grip and make sure we know what we’re doing before we enter the whitewater fast lane that typifies the real-time stream.  One Community. Many Voices. Create a free account to share your thoughts.  Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service.  We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines.  Please read the full list of posting rules found in our site's Terms of Service.